{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkBKubyZ7na7"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.877 · Análisis de sentimientos y textos</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster Universitario de Ciencia de Datos(Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis d'Informàtica, Multimèdia i Telecomunicacions</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjWsCAwv7na-"
   },
   "source": [
    "# PRA 3: Deep Learning para el análisis de textos \n",
    "\n",
    "En esta práctica revisaremos y aplicaremos los conocimientos aprendidos en los módulos 5 y 6. En concreto trataremos los siguientes temas:\n",
    "\n",
    "1. **Traducción automatica**: con custom embeddings y con embeddings preentrenados.\n",
    "2. **Classificación de frases**: Aplicación de los conceptos ya trabajados para la reutilización de la arquitectura de dos modelos.\n",
    "\n",
    "3. **Detección de NER y NEL**: detección y clasificación de entidades nombradas (NER) y entity linking basandonos en los temas ya trabajados en los notebooks de NER y NEL y añadiendo un ejemplo sencillo de transformers. \n",
    "\n",
    "También incluimos algunos otros temas transversales trabajados a lo largo de la asignatura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3Opj4kn7nbA"
   },
   "source": [
    "# PARTE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8DAwGy17nbB"
   },
   "source": [
    "En esta primera parte de la práctica se pide resolver los ejercicios usando la libreria **KERAS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyW13h9Z7nbB"
   },
   "source": [
    "# 1. Traducción Automática (4 puntos)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuKkXmmS7nbD"
   },
   "source": [
    "## 1.1 TA con Custom Embeddings (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuhhJrUk7nbD"
   },
   "source": [
    "\n",
    "El objetivo de este apartado es entrenar un modelo de traducción automática entre inglés y castellano, siguento los mismos pasos que en el notebook de Machine Translation del mòdulo 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdz0sUSb7nbD"
   },
   "source": [
    "\n",
    "**Implementación**: Siguiendo los pasos trabajados en el notebook de traducción automática, implementar y entrenar un modelo de traducción automática, del inglés al castellano.  \n",
    "\n",
    "+ La capa embedding debe de tener una dimensión igual a 300 \n",
    "+ Se recomienda una longitud màxima de secuencia de 12\n",
    "+ Se recomienda utilitzar los primeros 50000 pares del corpus de en-es.txt <br>\n",
    "    \n",
    "Mostrad la aplicación del modelo entrenado con algún ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "elapsed": 15116,
     "status": "ok",
     "timestamp": 1651784158435,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "U4hWr9Rv7nbF",
    "outputId": "87654de9-4fff-494c-cbdf-0948724c2d00"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L28axKt4kIwO"
   },
   "source": [
    "Primero deberéis cargar los datos proporcionados, que encontraréis en el fichero mt/en-es.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1652222332221,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "oVYldy9c7nbG"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fmFddVqkYU_"
   },
   "source": [
    "Preprocesar los datos, para eliminar puntuaciones y poner en minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1652222334072,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "JgIsoQqS7nbH"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRInE02Skhbv"
   },
   "source": [
    "Visualizar los datos resultantes, para tener una idea de como van a ser los datos con los que vamos a trabajar, en concreto ver el tamaño del corpus tanto los vectores del inglés como las del castellano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1652222337043,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "d8LfflHB7nbI"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1652222341520,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "QyM_eI7U7nbI"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3179,
     "status": "ok",
     "timestamp": 1651775127067,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "WPv_880u7nbJ",
    "outputId": "0978f1bd-3b6e-4c70-ee91-27611d87f9da"
   },
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16767,
     "status": "ok",
     "timestamp": 1651775147040,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "isjIX82C7nbK",
    "outputId": "7f0ae39a-bd9d-4abe-bf52-7f68a8b7e44c"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXqDyZiEks-S"
   },
   "source": [
    "Calcular el vocabulario tanto en castellano como en inglés, e imprimir su tamaño. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1652222347301,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "ofwfcS6f7nbL"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPXEAlbGoSuL"
   },
   "source": [
    "Separamos los conjuntos de entrenamiento por idioma y los codificamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1652222358924,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "mX7rtG4Q7nbM"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fYxSWIwoYSB"
   },
   "source": [
    "Definimos el modelo encoder-decoder basandonos en el notebook visto en la asignatura, e instanciamos el modelo con una capa de embedding para las frases de la lengua origen (castellano) y la dimensión de la última capa como el vocabulario de la lengua destino (inglés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1652222365667,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "bGLbPLtE7nbN"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuufosmqoxcA"
   },
   "source": [
    "Compilamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1652222369655,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "wNwpJh4D7nbO"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuVDwSApo0VT"
   },
   "source": [
    "Entrenamos y guardamos el modelo. \n",
    "El modelo puede tardar horas si se hace en CPU, mucho menos si se puede realizar en GPU. Colab permite el uso de GPU en general, si no se hace un uso extensivo, y se va deshabilitando la opción y habilitando segun necesidades. Si se tiene activada siempre penaliza y la desactiva. Para probar si funciona, recomendamos probar de lanzar el entrenamiento solo con una época y ver que funciona, y una vez tenemos claro que el flujo esta funcionando, ya lanzarlo con muchas más. \n",
    "\n",
    "Hemos visto que en Colab, a pesar de que pedimos que el tamaño de sentencia máxima sea 12, no puede cargar el modelo en memoria. Recomendamos bajarlo a 4 y el número de \"units\" a 200, de esta manera si que es capaz de trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1652222374146,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "_1muJyWc7nbO"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rnmfh3GpdJ0"
   },
   "source": [
    "Una vez entrenado el modelo, se aplica con el conjunto de test para obtener unas prediciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1652222379820,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "QX74wIEu7nbP"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Drwik5f5pllA"
   },
   "source": [
    "Visualizamos los resultados de las predicciones con los valores esperados. Los resultados son curiosos, no podríamos usar este modelo para un entorno real como vais a poder ver. \n",
    "\n",
    "Pregunta: ¿Porque creéis que no son buenos, y como creéis que podrían obtenerse mejores resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1652222386238,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "Se_zdVHz7nbP"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1652222390654,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "-_32oRE07nbP"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re2z6jLm7nbP"
   },
   "source": [
    "## 1.2 TA con Embeddings preentrenados (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zzbdDJq7nbP"
   },
   "source": [
    "En este apartado repetiremos el ejercicio anterior cargando a la capa de embedding los pesos d'un modelo GloVe entrenado para el inglés. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vncpo5D7nbP"
   },
   "source": [
    "Empezamos cargando el modelo GloVe para el inglés. Podéis usar 'glove.42B.300d.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19899,
     "status": "ok",
     "timestamp": 1651823037765,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "CKCaeep9B29Z",
    "outputId": "f39b138f-3c75-4db5-ad93-55d8c5bd3245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive/My Drive/Temporal\n",
      "Current working directory: /gdrive/My Drive/Temporal\n"
     ]
    }
   ],
   "source": [
    "# Podemos cargar el fichero glove desde Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive', force_remount=True)\n",
    "\n",
    "from pathlib import Path\n",
    "%cd /gdrive/My Drive/Temporal\n",
    "print (f\"Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135229,
     "status": "ok",
     "timestamp": 1651775463085,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "Oops18MJ7nbR",
    "outputId": "20aed401-c63e-495e-b400-a18e467157c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1917494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kajSHKSH7nbR"
   },
   "source": [
    "A continuación, tenemos que contruir la matriz de embeddings. \n",
    "Para no cargar todo el vocabulario del modelo, podemos filtrar solo aquellas entradas presentes en el vocabulario del tokenizador que usaremos. Además, tenemos de incluir en la matriz de vectores correspondientes los índices de las entradas (palabras) que no encontremos en el modelo glove cargado. Estos vectores se suelen inicializar con 0s o con el resultado de una distribución N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snHfP3Cb7nbR"
   },
   "source": [
    "Por ejemplo, si nuestro tokenizador se llamara `eng_tokenizer` podríamos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiQ797NU7nbR"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(eng_tokenizer.word_index) + 1, 300))\n",
    "for word, i in eng_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMMsDFOy7nbR"
   },
   "source": [
    "Para inicializar una capa de embeddings con pesos predefinidos se utiliza el argumento `weights`. Además, como no queremos que se modifiquen los pesos, marcamos el argumento `trainable` como `False`. \n",
    "\n",
    "Siguiendo con nuestro ejemplo, haríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLjfzHAq7nbR"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(eng_tokenizer.word_index) + 1,\n",
    "                            embedding_vec_length,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_text_length,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TAaf6I77nbR"
   },
   "source": [
    "Implementa y entrena de nuevo un modelo de traducción automática del inglés al castellano de forma similar, esta vez cargando los pesos de la capa embedding a partir del modelo Glove preentrenado en inglés y disponible en 'glove.42B.300d.txt'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1652222405004,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "9hKTQCdp7nbS"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqyHV6m3rIUn"
   },
   "source": [
    "Entrenamos y guardamos el modelo. Otra vez, aunque este entrenamiento es quizá un \"poco\" más liviano que el anterior, recomendamos el uso de GPU si es viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1652222412338,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "l58sRZaY7nbS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW-2Cnuqrbsu"
   },
   "source": [
    "Aplicar el modelo y visualizar los resultados a partir de las prediciones obtenidas con este nuevo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1652222422474,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "efztL7bg7nbT"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0n1SFEj7nbT"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>(Opcional) Análisis:</strong> Explica cuales són las principales diferencias entre los dos modelos entrenados. ¿Como podríamos mejorar los resultados de esta tarea en concreto?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcJ5JpIXuE9M"
   },
   "source": [
    "# PARTE 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpiJOV3MxHQQ"
   },
   "source": [
    "# 2. Classificación de frases (3 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO0kxolduE9M"
   },
   "source": [
    "En este apartado planteamos el uso de las arquitecturas vistas hasta el momento para crear un clasificador de notícias en inglés. \n",
    "En concreto usaremos este dataset:\n",
    "https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset/code\n",
    "Que clasifica les notícas en 4 tipos, \"world\", \"sports\", \"business\", y \"Science\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dACMx1zAN4qc"
   },
   "source": [
    "Lo que haremos primero será obtener el conjunto de datos de entrenamiento, cargarlo en un dataframe de pandas, seleccionar las columnas \"Title\" y \"Class Index\" y almacenar sus datos con el formato que utilizaremos para entrenar, 2 listas, la \"data\" y la \"data_labels\".\n",
    "\n",
    "En concreto las listas:\n",
    "\n",
    "1. **data**, contendrá todos los títulos de las noticias\n",
    "2. **data_labels**, todas las etiquetas.\n",
    "\n",
    "\n",
    "Ambas listas deben tener el mismo *tamaño*, ya que por un lado están las noticias, y cada noticia tiene una etiqueta.\n",
    "\n",
    "Se puede samplear el corpus y coger sólo las **10000** primeras entradas para que el script vaya más rápido. Más adelante podremos realizar pruebas para ver si añadiendo más entradas tenemos mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1651784390937,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "KHJEcnMwc9yB",
    "outputId": "0337fbb3-75c1-44f1-cf60-f43068139b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "train=pd.read_csv('class/train.csv')\n",
    "\n",
    "train_sample=train.sample(n=10000)\n",
    "title=train_sample['Title'].tolist()\n",
    "tag=train_sample['Class Index'].tolist()\n",
    "\n",
    "print (len(title))\n",
    "print (len(tag))\n",
    "\n",
    "data=[]\n",
    "data_labels=[]\n",
    "for i in range (0, len(title)):\n",
    "    data.append(title[i])\n",
    "    data_labels.append(tag[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eszZbTJRR2p1"
   },
   "source": [
    "Visualizad la distribución de textos por clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1652222428185,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "N6W_dKhLR19N"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mR6vfn3uE9N"
   },
   "source": [
    "Preparad y preprocesad los datos para el entrenamiento. Utilizaremos one-hot encoding por las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3080,
     "status": "ok",
     "timestamp": 1651784401680,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "-9egBY1Wc9yD",
    "outputId": "07d96904-d30d-4383-fc6b-6e8e2702635d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1651784404355,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "ga20obq5uE9N",
    "outputId": "6177c893-4fc3-472d-9a9f-3e8047af9410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ... 2 4 1]\n",
      "[0 1 1 ... 1 3 0]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "values = array(data_labels)\n",
    "print(values)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGh2TR3huE9N"
   },
   "source": [
    "## 2.1 Preparar datos y embeddings para entrenar (1 punto)\n",
    "La idea del modelo de clasificación que queremos implementar es más simple que la del encoder-decoder usado en el apartado 1.\n",
    "\n",
    "El modelo debe consistir sólo en:\n",
    "\n",
    "- una capa embedding con los pesos del modelo Glove preentrenado para el inglés disponible en el archivo 'glove.42B.300d.txt'\n",
    "- una capa LSTM con un número de units a elegir (por ejemplo, 300)\n",
    "- una capa Dense con una dimensión de salida que tiene el número de categorías con las que queremos clasificar (en este caso, 4).\n",
    "- Además, como loss function `loss` utilizaremos 'categorical_crossentropy' y como `optimizer`, 'adam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxxuaTTdawqX"
   },
   "source": [
    "Primeramente creamos un tokenizador para las frases del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1652222434290,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "kT12lHMJuE9O"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                               #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THxcbzAWuE9J"
   },
   "source": [
    "\n",
    "Cargamos el siguiente modelo GloVe para el inglés. Lo hemos utilizado en la parte 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1652222438246,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "WN5BNSvPuE9J"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                #\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg47gNQ2a9mZ"
   },
   "source": [
    "Una vez cargado el modelo de GloVe definimos la capa de Embedding con todos sus pesos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1652222442375,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "hXZn_3I9uE9O"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htUhNzKzbHcZ"
   },
   "source": [
    "Preparamos el corpus de entrenamiento y test, usando el model_selection de sklearn, y el onehot_encoded para las clases. Usamos 80% para train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1652222445571,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "NAba-FJvuE9O"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHwpK4hHgXwd"
   },
   "source": [
    "Codificar los vectores de entrada para el train y para el text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1652222449223,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "0HUX66RmuE9P"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_eqWNzNgi4e"
   },
   "source": [
    "## 2.2 Definir el modelo y entrenar (1 punto).\n",
    "\n",
    "El modelo debe consistir sólo en:\n",
    "\n",
    "+ una capa embedding con los pesos del modelo Glove preentrenado para el inglés disponible en el archivo 'glove.42B.300d.txt'\n",
    "+ una capa LSTM con un número de units a elegir (por ejemplo, 300)\n",
    "+ una capa Dense con una dimensión de salida que tiene el número de categorías con las que queremos clasificar (en este caso, 4).\n",
    "\n",
    "Además, como loss function utilizaremos 'categorical_crossentropy' y como optimizer, 'adam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1652222454275,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "0f-2I-K4uE9P"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpsPz5uDuZzV"
   },
   "source": [
    "Compilar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1652222458347,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "UC9BvysEuE9Q"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILf77Jy9ub5Z"
   },
   "source": [
    "Entrenar y guardar el modelo. En esta sección aunque sea recomendable usar GPU, con CPU también se puede obtener el resultado sin tener que esperar \"mucho\" tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1652222462704,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "0a5ez12buE9Q",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxKAcYjTurx2"
   },
   "source": [
    "## 2.3 Evaluar el modelo (1 punto)\n",
    "Se evalua el modelo y se obtienen sus diferentes métricas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1652222471221,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "EvXKuDAcuE9Q"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6BDFdI_RaaS"
   },
   "source": [
    "¿Que pasaría si el modelo de clasificación lo entrenaramos con más datos?\n",
    "¿Y si no usaramos los embeddings de Glove? ¿Que nos aportan ambas cosas?\n",
    "¿Si usamos más datos hacen falta usar los embeddings?\n",
    "\n",
    "Expresar vuestra opinión con una mínima experimentación para obtener las conclusiones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpRSYwVIwqZ1"
   },
   "source": [
    "# PARTE 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je-AFXAsuE9R"
   },
   "source": [
    "\n",
    "# 3. Detección de NER (3 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc5mcU77gWpe"
   },
   "source": [
    "En esta parte intentaremos entrenar y detectar entidades nombradas utilizando tanto SpaCy con transformers como transformers simplemente usando una librería llamada simple transformers.\n",
    "\n",
    "Por otro lado, también haremos Named Entity Linking (NEL) donde buscaremos entidades linkadas a una base de conocimiento (KB), en este caso DBpedia. Encontraremos los enlaces a Wikpedia de ciertas entidades del texto, utilizando DBPedia Spotlight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGB1-kYNxW5V"
   },
   "source": [
    "## 3.1 Detección de NER con spaCy (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIAzOTIEiH5I"
   },
   "source": [
    "En esta sección usaremos spaCy para detectar NER. A partir de un corpus de CONLL 2003, lo reeentrenaremos y de esta manera afinaremos su cobertura para estas clases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn9KhNzWlgjx"
   },
   "source": [
    "Instalamos spacy y mos modelos de lenguaje que necesitemos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43210,
     "status": "ok",
     "timestamp": 1651822961475,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "6j5WkL16iH5Q",
    "outputId": "a70f32e2-7f4d-4a6c-ee2e-125443e30c23"
   },
   "outputs": [],
   "source": [
    "!pip install spacy==3.2.0\n",
    "\n",
    "!python -m spacy download en_core_web_sm-3.2.0 --direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttkg0CqW4Ym9"
   },
   "source": [
    "Definimos un par de funciones que nos va a permitir imprimir los resultados de la detección de NER de forma muy interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8Ot3tH3lO48"
   },
   "outputs": [],
   "source": [
    "def get_tokens_to_print(model, text):\n",
    "  \"\"\"Print tokens of the text and its relevant attributes.\n",
    "\n",
    "    Parameters:\n",
    "      model (spaCy model): spaCy model used for tokenization\n",
    "      text (str):  text to transform in a spaCy doc class.\n",
    "\n",
    "    Returns: ---\n",
    "  \"\"\"\n",
    "  doc = model(text)\n",
    "  print (f\"The text:\\n\\n{get_text_to_print(text)}\\n\\nwas converted in a spaCy object: {type(doc)}\\n\")\n",
    "  print (f\"Token-based analysis. Each token is a spaCy object: {type(doc[0])}\\n\")\n",
    "  \n",
    "  # Obtener las filas para imprimir\n",
    "  rows  = []\n",
    "  # head_align: Lista de tuplas. Cada tupla: Cabecera de la columna y su alineación al imprimir\n",
    "  head_align  = [('Token', '<'), ('Lemma', '<'), ('Syntactic parent', '<'), ('#Tok', '>'), ('Chr_Start', '>'), ('Chr_End', '>'), ('POS', '<'), \n",
    "                 ('TAG', '<'), ('TAG meaning:', '<'), ('ENT', '<'), ('DEP', '<'), ('DEP meaning:', '<')]   \n",
    "  head, align = list(zip(*head_align))  \n",
    "  rows.append(head)                           \n",
    "  rows.append(['='*len(i) for i in head])     \n",
    "  for tok in doc:\n",
    "    rows.append([tok.text, tok.lemma_, tok.head.text, str(tok.i), str(tok.idx), str(tok.idx+len(tok)-1), tok.pos_, \n",
    "                 tok.tag_, str(spacy.explain(tok.tag_))[:20], tok.ent_type_, tok.dep_, str(spacy.explain(tok.dep_))[:20]])\n",
    "  \n",
    "  # Ancho de cada columna: igual al elemento más ancho de la columna\n",
    "  columns       = zip(*rows)     \n",
    "  column_widths = [max(len(i) for i in col) for col in columns]\n",
    "\n",
    "  # Imprimir \n",
    "  for row in rows:\n",
    "    print(*[f\"{row[i]:{align[i]}{column_widths[i]}}  \" for i in range(0, len(row))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9N8mQA3s56Z"
   },
   "outputs": [],
   "source": [
    "def get_text_to_print(text):\n",
    "  \"\"\"Format given text.\n",
    "\n",
    "    Parameters:\n",
    "      text (str): text to print\n",
    "\n",
    "    Returns:\n",
    "      str: text formatted in 100 character lines with an initial line numbering the characters\n",
    "  \"\"\"\n",
    "  line_length = 100\n",
    "  line_poss   = \"     1-------10--------20--------30--------40--------50--------60--------70--------80--------90-------100\"\n",
    "  text        = text.replace(\"\\n\", \" \")     \n",
    "  text        = text.replace(\"\\r\", \" \")     \n",
    "  text_format = \"\\n\".join([ f\"{i//line_length:<5}{text[i:i+line_length]}\"  for i in range(0, len(text), line_length) ])\n",
    "  return line_poss + \"\\n\" + text_format + \"\\n\" + line_poss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2xbabYm4uIe"
   },
   "source": [
    "Cargamos el modelo \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1652222513246,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "WNY5YFsUlYBN"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "## SOLUCION ####\n",
    "##################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GCpZoQd4oqD"
   },
   "source": [
    "Convertimos un texto en objecto 'Doc' de spaCy y visualizamos los resultados de analizar este texto a nivel de POS, NER, ENT, DEP... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1652222517732,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "iIDs4LbelUjj"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "## SOLUCION ####\n",
    "##################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHr-Nz6czi3P"
   },
   "source": [
    "Entrenar un nuevo modelo de NER con los datos de CONLL2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BHEexwQ2K5f"
   },
   "source": [
    "Convertimos los ficheros conll03 (train y valid) a formato spaCy. \n",
    "El corpus lo hemos obtenido de aquí:\n",
    "https://github.com/Hironsan/anago\n",
    "\n",
    "Nota: spacy contiene funciones que permiten convertir de formato conll al formato compilado que necesitan el módulo de train de spaCy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1652222524256,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "nYfj6_LVziVV"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### SOLUCION ###\n",
    "##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdXR3Jvg5kQV"
   },
   "source": [
    "Descargar el modelo 'en_core_web_trf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50915,
     "status": "ok",
     "timestamp": 1651823196955,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "CArCJmMSKm0H",
    "outputId": "c53a4ebf-c1d1-49a7-fba2-e49cc0a667d0"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10729,
     "status": "ok",
     "timestamp": 1651823215115,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "AKBnk2lcKlWZ",
    "outputId": "93540ba7-21f3-4238-9b7f-8b26bd94bf70"
   },
   "outputs": [],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1651823216538,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "NAA9QpGWKosp",
    "outputId": "2af496b8-a2b5-4009-e4fb-c2beb6c4dd2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy version installed: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print (f\"Spacy version installed: {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L35Q-Df_5yfC"
   },
   "source": [
    "Entrenar usando la función train de spaCy a partir del modelo 'en_core_web_trf'. \n",
    "Usar el fichero de configuración adjunto y modificar las cosas que consideréis oportundas. La versión entregada ya funciona pero se puede customizar si hay interés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1652222534542,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "iXl4T4595d1x"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "### SOLUCION ###\n",
    "################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOlpHvLn6Qfg"
   },
   "source": [
    "Predecir una frase de ejemplo con el nuevo modelo y visualizar los resultados. \n",
    "Ojo que colab, no le gusta cargar modelos desde paths, solo si estan en local. con lo que recomendamos, generar el modelo en drive, guardarlo y luego subir la mejor versión para cargarlo desde de aquí. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmr1vYmY_MUC"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "### SOLUCION ###\n",
    "################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TR08_0r7dIT"
   },
   "source": [
    "Evaluar los resultados obtenidos y calcular las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1652222557308,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "cJqZUm9k7gNb"
   },
   "outputs": [],
   "source": [
    "############ \n",
    "###SOLUCION##\n",
    "##############\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvK065XrdF57"
   },
   "source": [
    "## 3.2 Detección y entrenamiento de NER con Simple Transformers (0.5 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CS3j48xr08m9"
   },
   "source": [
    "Probaremos de entrenar un modelo también haciendo uso de Transformers para la detección de NER pero sin usar SPACY. \n",
    "En este caso, usaremos Simple Transformers, la documentación está aquí:\n",
    "\n",
    "https://simpletransformers.ai/docs/ner-model/\n",
    "\n",
    "Es una manera de generar modelos sencilla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIR17xv3iH5I"
   },
   "source": [
    "En primera instancia prepararemos el dataset que utilizaremos. Utilizaremos el CONLL2003.\n",
    "El corpus lo hemos obtenido de aquí:\n",
    "https://github.com/Hironsan/anago\n",
    "\n",
    "El dataset debe estar como una lista de listas. Donde cada lista contiene el número de la frase, el token y si es NER o no en el formato BIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1651829876934,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "QqbykeS0iH5J",
    "outputId": "7c3879e0-64f1-43b1-9a5d-fe316a553c0d"
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset):\n",
    "    f = open(dataset, \"r\")\n",
    "    num_sentence=0\n",
    "\n",
    "    doc=[]\n",
    "    for i in f:\n",
    "\n",
    "        frase=i.strip()\n",
    "  \n",
    "        if (num_sentence < 1500):\n",
    "            if len(frase)==0:\n",
    "                num_sentence=num_sentence+1\n",
    "\n",
    "            else:\n",
    "                #print (\"no empty\", frase)\n",
    "                dos=frase.split(\"\\t\")\n",
    "\n",
    "                if (len(dos)==2):\n",
    "                    #print (sentence, dos[0], dos[1])\n",
    "                    l=[num_sentence, dos[0],dos[1]]\n",
    "                    doc.append(l)\n",
    "    print (num_sentence)            \n",
    "    return doc\n",
    "\n",
    "train_data=create_dataset(\"ner/conll03/train.txt\")\n",
    "eval_data=create_dataset(\"ner/conll03/test.txt\")\n",
    "print (eval_data)\n",
    "print (len(eval_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0ykR3spiH5M"
   },
   "source": [
    "Convertiremos el dataset con las mismas columnas que hay en los archivos de train/test a un dataframe de Pandas. Donde está el identificador de la frase, palabra, etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1652222564201,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "zDJgjrAXiH5M"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "##### SOLUCION #####\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIQGfnd0iH5M"
   },
   "source": [
    "Importamos el paquete simple transformers y creamos un modelo NER, basado en bert, y con los siguientes parámetros como argumentos:\n",
    "\"overwrite_output_dir\": True\n",
    "\"reprocess_input_data\": True\n",
    "\n",
    "Le decimos que no use GPU (use_cuda=False) en caso de no tener disponible. En caso de tener GPU disponible podemos indicar use_cuda=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1652222569530,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "nOYYRHoLiH5N"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "####### SOLUCION #######\n",
    "##################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6bNN_x3iH5N"
   },
   "source": [
    "Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1652222574977,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "V4VrQDdniH5O"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "####### SOLUCION #######\n",
    "##################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_u6qRtQiH5O"
   },
   "source": [
    "Evaluamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1652222580760,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "-Amjh5NRiH5P"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "####### SOLUCION #######\n",
    "##################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiNqYfLDiH5P"
   },
   "source": [
    "Imprimimos los resultados y vemos algun ejemplo para ver que pinta tiene el uso de este modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1652222587246,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "_dSjAF4BiH5Q"
   },
   "outputs": [],
   "source": [
    "################################\n",
    "####### SOLUCION #######\n",
    "##################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-o-GEIf2-Bv"
   },
   "source": [
    "Que diferencias claras ves entre este modelo y el modelo anterior donde se ha usado SPACY? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdBh-tsniQVp"
   },
   "source": [
    "## 3.3 NEL (0.5 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA--p0Q4kjID"
   },
   "source": [
    "En esta sección, la idea es obtener los enlaces en la DBpedia spotlight relacionados con las entidades que se han obtenido de NER usando SPACY. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUqk2jImvXRi"
   },
   "source": [
    "\n",
    "Desarrolla una función que dado un texto, te obtenga automáticamente las entidades relacionadas en el DBpedia Spotlight.\n",
    "\n",
    "URL de acceso a la API, DBPedia inglés: https://www.dbpedia-spotlight.org/api o https://www.dbpedia-spotlight.org/api/en  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1652222596962,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "-MAGBBL0iPdZ"
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# SOLUCIÓN #\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKV1Bqswc8WD"
   },
   "source": [
    "Prepara una función que a acceda a la función desarrollada anteriormente y analize un texto, teniendo en cuenta que se pueden configurar estos parametros:\n",
    "\n",
    "\n",
    "+ confidence (float): score de confianza para la desambiguación\n",
    "+ support (int): relevancia de la entidad, basada en los links de Wikipedia\n",
    "+ dbpedia_lan (str): idioma de la wikipedia\n",
    "+ list_types (list): listado de tipos que se van a filtar de acuerdo con la ontologia de la dbpedia(rdf:type). La lista puede ser vacía.\n",
    "+ base_url: url DBPedia Spotlight API\n",
    "+ action: acción definida en la API de la DBpedia (anotar, spot, candidates)\n",
    "\n",
    "Todos estos parametros son parametros que coge la API de DBpedia, no hay que hacer ningún calculo especial, simplemente añadir la opción de poder jugar con ellos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wc-qgu4QiToE"
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# SOLUCIO #\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4178,
     "status": "ok",
     "timestamp": 1651832518359,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "aou0QsDkkYYB",
    "outputId": "233159b2-3434-40f9-9493-62a21d28fab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNY7QHEZdFL0"
   },
   "source": [
    "A partir de un texto de ejemplo, calcular los enlaces de DBpedia y visualizarlos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1652222627649,
     "user": {
      "displayName": "Montse Cuadros Oller",
      "userId": "03990437333465779187"
     },
     "user_tz": -120
    },
    "id": "ehDbnac7irW0"
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# SOLUCIÓN #\n",
    "########################\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PRA3_master_es.ipynb",
   "provenance": [
    {
     "file_id": "1aRFkj_b0EBynWe63q1DlklJcKAeHXbVS",
     "timestamp": 1651095028290
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('my_venv_NLP': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ba57faba181d11e3f50ff79199987f725c18f7d420f734e5a8ae6cfc45462ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
